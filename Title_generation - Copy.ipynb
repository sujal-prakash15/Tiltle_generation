{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLBJXpX3DmjY"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C1PuvwJJDOL4"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pickle \n",
    "def get_logger(name, log_file=None):\n",
    "    format = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "    if not log_file:\n",
    "        handle = logging.StreamHandler()\n",
    "    else:\n",
    "        handle = logging.FileHandler(log_file)\n",
    "    handle.setFormatter(format)\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.addHandler(handle)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def load_pkl(pkl_path):\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "    return result\n",
    "\n",
    "\n",
    "def dump_pkl(vocab, pkl_path, overwrite=True):\n",
    "    if os.path.exists(pkl_path) and not overwrite:\n",
    "        return\n",
    "    with open(pkl_path, 'wb') as f:\n",
    "        # pickle.dump(vocab, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pickle.dump(vocab, f, protocol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YruFu20rDerX"
   },
   "outputs": [],
   "source": [
    "# segementation part\n",
    "def edit_distance_word(word, char_set):\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in char_set]\n",
    "    return set(transposes + replaces)\n",
    "\n",
    "\n",
    "def get_sub_array(nums):\n",
    "    ret = []\n",
    "    ii = 0\n",
    "    for i, c in enumerate(nums):\n",
    "        if i == 0:\n",
    "            pass\n",
    "        elif i <= ii:\n",
    "            continue\n",
    "        elif i == len(nums) - 1:\n",
    "            ret.append([c])\n",
    "            break\n",
    "        ii = i\n",
    "        cc = c\n",
    "        # get continuity Substring\n",
    "        while ii < len(nums) - 1 and nums[ii + 1] == cc + 1:\n",
    "            ii = ii + 1\n",
    "            cc = cc + 1\n",
    "        if ii > i:\n",
    "            ret.append([c, nums[ii] + 1])\n",
    "        else:\n",
    "            ret.append([c])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EH9iCjGJEZeU",
    "outputId": "c235655b-1aa1-45d2-87a9-0283488c69a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nihao True\n",
      "  　\n",
      "! ！\n",
      "\" ＂\n",
      "# ＃\n",
      "$ ＄\n",
      "% ％\n",
      "& ＆\n",
      "' ＇\n",
      "( （\n",
      ") ）\n",
      "* ＊\n",
      "+ ＋\n",
      ", ，\n",
      "- －\n",
      ". ．\n",
      "/ ／\n",
      "0 ０\n",
      "1 １\n",
      "2 ２\n",
      "3 ３\n",
      "4 ４\n",
      "5 ５\n",
      "6 ６\n",
      "7 ７\n",
      "8 ８\n",
      "9 ９\n",
      ": ：\n",
      "; ；\n",
      "< ＜\n",
      "= ＝\n",
      "> ＞\n",
      "? ？\n",
      "@ ＠\n",
      "A Ａ\n",
      "B Ｂ\n",
      "C Ｃ\n",
      "D Ｄ\n",
      "E Ｅ\n",
      "F Ｆ\n",
      "G Ｇ\n",
      "H Ｈ\n",
      "I Ｉ\n",
      "J Ｊ\n",
      "K Ｋ\n",
      "L Ｌ\n",
      "M Ｍ\n",
      "N Ｎ\n",
      "O Ｏ\n",
      "P Ｐ\n",
      "Q Ｑ\n",
      "R Ｒ\n",
      "S Ｓ\n",
      "T Ｔ\n",
      "U Ｕ\n",
      "V Ｖ\n",
      "W Ｗ\n",
      "X Ｘ\n",
      "Y Ｙ\n",
      "Z Ｚ\n",
      "[ ［\n",
      "\\ ＼\n",
      "] ］\n",
      "^ ＾\n",
      "_ ＿\n",
      "` ｀\n",
      "a ａ\n",
      "b ｂ\n",
      "c ｃ\n",
      "d ｄ\n",
      "e ｅ\n",
      "f ｆ\n",
      "g ｇ\n",
      "h ｈ\n",
      "i ｉ\n",
      "j ｊ\n",
      "k ｋ\n",
      "l ｌ\n",
      "m ｍ\n",
      "n ｎ\n",
      "o ｏ\n",
      "p ｐ\n",
      "q ｑ\n",
      "r ｒ\n",
      "s ｓ\n",
      "t ｔ\n",
      "u ｕ\n",
      "v ｖ\n",
      "w ｗ\n",
      "x ｘ\n",
      "y ｙ\n",
      "z ｚ\n",
      "{ ｛\n",
      "| ｜\n",
      "} ｝\n",
      "~ ～\n",
      "中国 人名a高频a  扇\n",
      "True\n",
      "你干么!d7&888学英 语abc?nz\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "from jieba import posseg\n",
    "def is_chinese(uchar):\n",
    "    if '\\u4e00' <= uchar <= '\\u9fa5':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_chinese_string(string):\n",
    "    for c in string:\n",
    "        if not is_chinese(c):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def is_number(uchar):\n",
    "    if u'u0030' <= uchar <= u'u0039':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_alphabet(uchar):\n",
    "    if (u'u0041' <= uchar <= u'u005a') or (u'u0061' <= uchar <= u'u007a'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_alphabet_string(string):\n",
    "    for c in string:\n",
    "        if c < 'a' or c > 'z':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def is_other(uchar):\n",
    "    if not (is_chinese(uchar) or is_number(uchar) or is_alphabet(uchar)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def B2Q(uchar):\n",
    "    inside_code = ord(uchar)\n",
    "    if inside_code < 0x0020 or inside_code > 0x7e:  # 不是半角字符就返回原来的字符\n",
    "        return uchar\n",
    "    if inside_code == 0x0020:  # 除了空格其他的全角半角的公式为:半角=全角-0xfee0\n",
    "        inside_code = 0x3000\n",
    "    else:\n",
    "        inside_code += 0xfee0\n",
    "    return chr(inside_code)\n",
    "\n",
    "\n",
    "def Q2B(uchar):\n",
    "    inside_code = ord(uchar)\n",
    "    if inside_code == 0x3000:\n",
    "        inside_code = 0x0020\n",
    "    else:\n",
    "        inside_code -= 0xfee0\n",
    "    if inside_code < 0x0020 or inside_code > 0x7e:  # 转完之后不是半角字符返回原来的字符\n",
    "        return uchar\n",
    "    return chr(inside_code)\n",
    "\n",
    "\n",
    "def stringQ2B(ustring):\n",
    "    return \"\".join([Q2B(uchar) for uchar in ustring])\n",
    "\n",
    "\n",
    "def uniform(ustring):\n",
    "    return stringQ2B(ustring).lower()\n",
    "\n",
    "\n",
    "def remove_punctuation(strs):\n",
    "    return re.sub(\"[\\s+\\.\\!\\/<>“”,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\", strs.strip())\n",
    "\n",
    "\n",
    "def segment(sentence, cut_type='word', pos=False):\n",
    "    import logging\n",
    "    jieba.default_logger.setLevel(logging.ERROR)\n",
    "    if pos:\n",
    "        if cut_type == 'word':\n",
    "            word_pos_seq = posseg.lcut(sentence)\n",
    "            word_seq, pos_seq = [], []\n",
    "            for w, p in word_pos_seq:\n",
    "                word_seq.append(w)\n",
    "                pos_seq.append(p)\n",
    "            return word_seq, pos_seq\n",
    "        elif cut_type == 'char':\n",
    "            word_seq = list(sentence)\n",
    "            pos_seq = []\n",
    "            for w in word_seq:\n",
    "                w_p = posseg.lcut(w)\n",
    "                pos_seq.append(w_p[0].flag)\n",
    "            return word_seq, pos_seq\n",
    "    else:\n",
    "        if cut_type == 'word':\n",
    "            return jieba.lcut(sentence)\n",
    "        elif cut_type == 'char':\n",
    "            return list(sentence)\n",
    "\n",
    "\n",
    "def tokenize(sentence, mode='default'):\n",
    "    import logging\n",
    "    jieba.default_logger.setLevel(logging.ERROR)\n",
    "    return list(jieba.tokenize(sentence, mode=mode))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    a = 'nihao'\n",
    "    print(a, is_alphabet_string(a))\n",
    "    # test Q2B and B2Q\n",
    "    for i in range(0x0020, 0x007F):\n",
    "        print(Q2B(B2Q(chr(i))), B2Q(chr(i)))\n",
    "    # test uniform\n",
    "    ustring = '中国 人名ａ高频Ａ  扇'\n",
    "    ustring = uniform(ustring)\n",
    "    print(ustring)\n",
    "    print(is_other(','))\n",
    "    print(uniform('你干么！ｄ７＆８８８学英 语ＡＢＣ？ｎｚ'))\n",
    "    print(is_chinese('喜'))\n",
    "    print(is_chinese_string('喜,'))\n",
    "    print(is_chinese_string('丽，'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMhoblftDs1I"
   },
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tkMymYPTD0Rh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = './data'\n",
    "train_path = os.path.join(data_dir, 'train_sample.txt')\n",
    "test_path = os.path.join(data_dir, 'test_sample.txt')\n",
    "output_dir = './output'\n",
    "save_vocab_path = os.path.join(output_dir, 'vocab.txt')\n",
    "attn_model_path = os.path.join(output_dir, 'attn_model.weight')\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "rnn_hidden_dim = 128\n",
    "maxlen = 400\n",
    "min_count = 5\n",
    "dropout = 0.0\n",
    "use_gpu = False\n",
    "sep = '\\t'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsS0Vyq_EEa7"
   },
   "source": [
    "## reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tzeRmp4dEeyB"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "PAD_TOKEN = 'PAD'\n",
    "EOS_TOKEN = 'EOS'\n",
    "GO_TOKEN = 'GO'\n",
    "UNK_TOKEN = 'UNK'\n",
    "\n",
    "\n",
    "class Reader(object):\n",
    "    def __init__(self, train_path=None, token_2_id=None,\n",
    "                 special_tokens=(), min_count=1, sep='\\t'):\n",
    "        if token_2_id:\n",
    "            self.token_2_id = token_2_id\n",
    "        else:\n",
    "            token_counts = defaultdict(int)\n",
    "            for tokens in self.read_tokens(train_path):\n",
    "                for i in tokens:\n",
    "                    token_counts[i] += 1\n",
    "            new_token_counts = {}\n",
    "            for i, j in token_counts.items():\n",
    "                if j >= min_count:\n",
    "                    new_token_counts[i] = j\n",
    "            self.token_counts = new_token_counts\n",
    "            count_pairs = sorted(self.token_counts.items(), key=lambda k: (-k[1], k[0]))\n",
    "            vocab, _ = list(zip(*count_pairs))\n",
    "            vocab = list(vocab)\n",
    "            vocab[0:0] = special_tokens\n",
    "            full_token_id = list(zip(vocab, range(len(vocab))))\n",
    "            self.token_2_id = dict(full_token_id)\n",
    "        self.id_2_token = {int(v): k for k, v in self.token_2_id.items()}\n",
    "        self.sep = sep\n",
    "\n",
    "    def read_tokens(self, path):\n",
    "       \n",
    "        raise NotImplementedError(\"Must implement read_tokens\")\n",
    "\n",
    "    def unknown_token(self):\n",
    "        raise NotImplementedError(\"Must implement unknow_tokens\")\n",
    "\n",
    "    def read_samples_by_string(self, path):\n",
    "        raise NotImplementedError(\"Must implement read_samples\")\n",
    "\n",
    "    def convert_token_2_id(self, token):\n",
    "        token_id = token if token in self.token_2_id else self.unknown_token()\n",
    "        return self.token_2_id[token_id]\n",
    "\n",
    "    def convert_id_2_token(self, id):\n",
    "        return self.id_2_token[id]\n",
    "\n",
    "    def is_unknown_token(self, token):\n",
    "        return token not in self.token_2_id or token == self.unknown_token()\n",
    "\n",
    "    def sentence_2_token_ids(self, sentence):\n",
    "        return [self.convert_token_2_id(w) for w in sentence.split()]\n",
    "\n",
    "    def token_ids_2_tokens(self, word_ids):\n",
    "        return [self.convert_id_2_token(w) for w in word_ids]\n",
    "\n",
    "    def read_samples(self, path):\n",
    "        for source_words, target_words in self.read_samples_by_string(path):\n",
    "            source = [self.convert_token_2_id(w) for w in source_words]\n",
    "            target = [self.convert_token_2_id(w) for w in target_words]\n",
    "            # head: \"GO\"; last: \"EOS\"\n",
    "            target.insert(0, GO_ID)\n",
    "            target.append(EOS_ID)\n",
    "            yield source, target\n",
    "\n",
    "    def read_samples_tokens(self, path):\n",
    "        for source_words, target_words in self.read_samples_by_string(path):\n",
    "            target = target_words\n",
    "            # head: \"GO\"; last: \"EOS\"\n",
    "            target.insert(0, GO_TOKEN)\n",
    "            target.append(EOS_TOKEN)\n",
    "            yield source_words, target\n",
    "\n",
    "    def build_dataset(self, path):\n",
    "        print('Read data, path:{0}'.format(path))\n",
    "        sources, targets = [], []\n",
    "        for source, target in self.read_samples_tokens(path):\n",
    "            sources.append(source)\n",
    "            targets.append(target)\n",
    "        return sources, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2LJ1HqDElvu"
   },
   "source": [
    "## corpus_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "j6pAoU8gEIzr"
   },
   "outputs": [],
   "source": [
    "from codecs import open\n",
    "\n",
    "# from generator.utils.io_utils import get_logger\n",
    "# from generator.reader import Reader, PAD_TOKEN, EOS_TOKEN, GO_TOKEN, UNK_TOKEN\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "def save_word_dict(dict_data, save_path):\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        for k, v in dict_data.items():\n",
    "            f.write(\"%s\\t%d\\n\" % (k, v))\n",
    "\n",
    "\n",
    "def load_word_dict(save_path):\n",
    "    dict_data = dict()\n",
    "    with open(save_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            items = line.strip().split()\n",
    "            try:\n",
    "                dict_data[items[0]] = int(items[1])\n",
    "            except IndexError:\n",
    "                logger.error('error', line)\n",
    "    return dict_data\n",
    "\n",
    "\n",
    "class CorpusReader(Reader):\n",
    "    def __init__(self, train_path=None, token_2_id=None, min_count=1, sep='\\t'):\n",
    "        super(CorpusReader, self).__init__(\n",
    "            train_path=train_path,\n",
    "            token_2_id=token_2_id,\n",
    "            special_tokens=[PAD_TOKEN, GO_TOKEN, EOS_TOKEN, UNK_TOKEN],\n",
    "            min_count=min_count,\n",
    "            sep=sep)\n",
    "        self.UNKNOWN_ID = self.token_2_id[UNK_TOKEN]\n",
    "\n",
    "    def read_samples_by_string(self, path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            while True:\n",
    "                line = f.readline()\n",
    "                line = line.lower().strip()\n",
    "                if not line:\n",
    "                    break\n",
    "                if self.sep not in line:\n",
    "                    continue\n",
    "                source, target = line.split(self.sep)\n",
    "                yield source.split(), target.split()\n",
    "\n",
    "    def unknown_token(self):\n",
    "        return UNK_TOKEN\n",
    "\n",
    "    def read_tokens(self, path, is_infer=False):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                yield line.lower().strip().split()\n",
    "\n",
    "\n",
    "def str2id(s, char2id, maxlen):\n",
    "    return [char2id.get(c, char2id[UNK_TOKEN]) for c in s[:maxlen]]\n",
    "\n",
    "\n",
    "def padding(x, char2id):\n",
    "    ml = max([len(i) for i in x])\n",
    "    return [i + [char2id[PAD_TOKEN]] * (ml - len(i)) for i in x]\n",
    "\n",
    "\n",
    "def id2str(ids, id2char):\n",
    "    return ''.join([id2char.get(i, '') for i in ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umDaXgPGEV2X"
   },
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4hxb5qYqEbjF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\sujal\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.501 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people 0.38072507971019104\n",
      "was 0.38072507971019104\n",
      "their 0.30458006376815283\n",
      "crime 0.30458006376815283\n",
      "rachel 0.30458006376815283\n",
      "she 0.30458006376815283\n",
      "her 0.30458006376815283\n",
      "what 0.30458006376815283\n",
      "when 0.22843504782611462\n",
      "they 0.22843504782611462\n",
      "own 0.22843504782611462\n",
      "someone 0.15229003188407642\n",
      "murder 0.15229003188407642\n",
      "go 0.15229003188407642\n",
      "most 0.15229003188407642\n",
      "come 0.15229003188407642\n",
      "other 0.15229003188407642\n",
      "do 0.15229003188407642\n",
      "just 0.15229003188407642\n",
      "mother 0.15229003188407642\n",
      "******************************************\n",
      "her 1.0\n",
      "people 0.939535544696493\n",
      "she 0.9050636065888443\n",
      "their 0.8998168470055914\n",
      "they 0.8474956845601105\n",
      "what 0.7728483618043107\n",
      "was 0.7553193797252628\n",
      "rachel 0.7482045941740926\n",
      "when 0.5876266445450075\n",
      "own 0.5749351275800411\n",
      "go 0.4933802679645524\n",
      "come 0.4781172142187957\n",
      "crime 0.45992833107989767\n",
      "act 0.4269534435942861\n",
      "just 0.4231355960295955\n",
      "your 0.41912524309343174\n",
      "mother 0.39352575584470395\n",
      "number 0.38389680194447895\n",
      "gain 0.3809415453547345\n",
      "scrutiny 0.37592895526102865\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "sentence = 'when someone commits a murder they typically go to extreme lengths to cover up their brutal crime . the harsh prison sentences that go along with killing someone are enough to deter most people from ever wanting to be caught , not to mention the intense social scrutiny they would face . occasionally , however , there are folks who come forward and admit guilt in their crime . this can be for any number of reasons , like to gain notoriety or to clear their conscience , though , in other instances , people do it to come clean to the people they care about . when rachel hutson was just 19 years old , she murdered her own mother in cold blood . as heinous and unimaginable as her crime was , it was what she did after that shocked people the most … rachel was just a teenager when she committed an unthinkable act against her own other … while that in and of itself was a heinous crime , it ’s what rachel did in the aftermath of her own mother ’s murder that shook people to their core . you ’re not going to believe what strange thing she decided to do next … it ’s hard to understand what drove rachel to commit this terrible act , but sending the photo afterward seems to make even less sense . share this heartbreaking story with your friends below .'\n",
    "keywords = jieba.analyse.extract_tags(sentence, topK=20, withWeight=True)\n",
    "\n",
    "for item in keywords:\n",
    "    print(item[0], item[1])\n",
    "print('*' * 42)\n",
    "keywords = jieba.analyse.textrank(sentence, topK=20, withWeight=True, allowPOS=('n', 'nr', 'ns','eng'))\n",
    "for item in keywords:\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0NNEKXYEZeo"
   },
   "source": [
    "# sequential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7tAdm-B-EZep"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Lambda, Layer, Embedding, Bidirectional, Dense, Activation, GRU, CuDNNGRU\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "class ScaleShift(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ScaleShift, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        kernel_shape = (1,) * (len(input_shape) - 1) + (input_shape[-1],)\n",
    "        self.log_scale = self.add_weight(name='log_scale',\n",
    "                                         shape=kernel_shape,\n",
    "                                         initializer='zeros')\n",
    "        self.shift = self.add_weight(name='shift',\n",
    "                                     shape=kernel_shape,\n",
    "                                     initializer='zeros')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_outs = K.exp(self.log_scale) * inputs + self.shift\n",
    "        return x_outs\n",
    "\n",
    "\n",
    "class Interact(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Interact, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        in_dim = input_shape[0][-1]\n",
    "        out_dim = input_shape[1][-1]\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(in_dim, out_dim),\n",
    "                                      initializer='glorot_normal')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        q, v, v_mask = inputs\n",
    "        k = v\n",
    "        mv = K.max(v - (1. - v_mask) * 1e10, axis=1, keepdims=True)  # maxpooling1d\n",
    "        mv = mv + K.zeros_like(q[:, :, :1]) \n",
    "        qw = K.dot(q, self.kernel)\n",
    "        a = K.batch_dot(qw, k, [2, 2]) / 10.\n",
    "        a -= (1. - K.permute_dimensions(v_mask, [0, 2, 1])) * 1e10\n",
    "        a = K.softmax(a)\n",
    "        o = K.batch_dot(a, v, [2, 1])\n",
    "        return K.concatenate([o, q, mv], 2)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, input_shape[0][1],\n",
    "                input_shape[0][2] + input_shape[1][2] * 2)\n",
    "\n",
    "\n",
    "class Seq2seqAttnModel(object):\n",
    "    def __init__(self, chars, hidden_dim=128, attn_model_path=None, use_gpu=False, dropout=0.2):\n",
    "        self.chars = chars\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.model_path = attn_model_path\n",
    "        self.use_gpu = use_gpu\n",
    "        self.dropout = float(dropout)\n",
    "\n",
    "    def build_model(self):\n",
    "        x_in = Input(shape=(None,))\n",
    "        y_in = Input(shape=(None,))\n",
    "        x = x_in\n",
    "        y = y_in\n",
    "        x_mask = Lambda(lambda x: K.cast(K.greater(K.expand_dims(x, 2), 0), 'float32'))(x)\n",
    "        y_mask = Lambda(lambda x: K.cast(K.greater(K.expand_dims(x, 2), 0), 'float32'))(y)\n",
    "\n",
    "        x_one_hot = Lambda(self._one_hot)([x, x_mask])\n",
    "        x_prior = ScaleShift()(x_one_hot)\n",
    "\n",
    "        # embedding\n",
    "        embedding = Embedding(len(self.chars), self.hidden_dim)\n",
    "        x = embedding(x)\n",
    "        y = embedding(y)\n",
    "\n",
    "        if self.use_gpu:\n",
    "            # encoder\n",
    "            x = Bidirectional(CuDNNGRU(int(self.hidden_dim / 2), return_sequences=True))(x)\n",
    "            x = Bidirectional(CuDNNGRU(int(self.hidden_dim / 2), return_sequences=True))(x)\n",
    "            # decoder\n",
    "            y = CuDNNGRU(self.hidden_dim, return_sequences=True)(y)\n",
    "            y = CuDNNGRU(self.hidden_dim, return_sequences=True)(y)\n",
    "        else:\n",
    "            # encoder\n",
    "            x = Bidirectional(GRU(int(self.hidden_dim / 2), return_sequences=True, dropout=self.dropout))(x)\n",
    "            x = Bidirectional(GRU(int(self.hidden_dim / 2), return_sequences=True, dropout=self.dropout))(x)\n",
    "            # decoder\n",
    "            y = GRU(self.hidden_dim, return_sequences=True, dropout=self.dropout)(y)\n",
    "            y = GRU(self.hidden_dim, return_sequences=True, dropout=self.dropout)(y)\n",
    "\n",
    "        xy = Interact()([y, x, x_mask])\n",
    "        xy = Dense(512, activation='relu')(xy)\n",
    "        xy = Dense(len(self.chars))(xy)\n",
    "        xy = Lambda(lambda x: (x[0] + x[1]) / 2)([xy, x_prior]) \n",
    "        xy = Activation('softmax')(xy)\n",
    "\n",
    "        cross_entropy = K.sparse_categorical_crossentropy(y_in[:, 1:], xy[:, :-1])\n",
    "        loss = K.sum(cross_entropy * y_mask[:, 1:, 0]) / K.sum(y_mask[:, 1:, 0])\n",
    "\n",
    "        model = Model([x_in, y_in], xy)\n",
    "        model.add_loss(loss)\n",
    "        model.compile(optimizer=Adam(1e-3))\n",
    "        if os.path.exists(self.model_path):\n",
    "            model.load_weights(self.model_path)\n",
    "        return model\n",
    "\n",
    "    def _one_hot(self, x):\n",
    "        x, x_mask = x\n",
    "        x = K.cast(x, 'int32')\n",
    "        x = K.one_hot(x, len(self.chars))\n",
    "        x = K.sum(x_mask * x, 1, keepdims=True)\n",
    "        x = K.cast(K.greater(x, 0.5), 'float32')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0D3_PCvEK5T"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "# from generator.corpus_reader import str2id, id2str\n",
    "# from generator.reader import GO_TOKEN, EOS_TOKEN\n",
    "\n",
    "\n",
    "def gen_target(input_text, model, char2id, id2char, maxlen=400, topk=3, max_target_len=50):\n",
    "    xid = np.array([str2id(input_text, char2id, maxlen)] * topk) \n",
    "    yid = np.array([[char2id[GO_TOKEN]]] * topk)\n",
    "    scores = [0] * topk\n",
    "    for i in range(max_target_len):\n",
    "        proba = model.predict([xid, yid])[:, i, :]\n",
    "        log_proba = np.log(proba + 1e-6)\n",
    "        arg_topk = log_proba.argsort(axis=1)[:, -topk:]\n",
    "        _yid = []\n",
    "        _scores = []\n",
    "        if i == 0:\n",
    "            for j in range(topk):\n",
    "                _yid.append(list(yid[j]) + [arg_topk[0][j]])\n",
    "                _scores.append(scores[j] + log_proba[0][arg_topk[0][j]])\n",
    "        else:\n",
    "            for j in range(len(xid)):\n",
    "                for k in range(topk):\n",
    "                    _yid.append(list(yid[j]) + [arg_topk[j][k]])\n",
    "                    _scores.append(scores[j] + log_proba[j][arg_topk[j][k]])\n",
    "            _arg_topk = np.argsort(_scores)[-topk:]\n",
    "            _yid = [_yid[k] for k in _arg_topk]\n",
    "            _scores = [_scores[k] for k in _arg_topk]\n",
    "        yid = []\n",
    "        scores = []\n",
    "        for k in range(len(xid)):\n",
    "            if _yid[k][-1] == char2id[EOS_TOKEN]:\n",
    "                return id2str(_yid[k][1:-1], id2char)\n",
    "            else:\n",
    "                yid.append(_yid[k])\n",
    "                scores.append(_scores[k])\n",
    "        yid = np.array(yid)\n",
    "    return id2str(yid[np.argmax(scores)][1:-1], id2char)\n",
    "\n",
    "\n",
    "class Evaluate(Callback):\n",
    "    def __init__(self, model, attn_model_path, char2id, id2char, maxlen):\n",
    "        super(Evaluate, self).__init__()\n",
    "        self.lowest = 1e10\n",
    "        self.model = model\n",
    "        self.attn_model_path = attn_model_path\n",
    "        self.char2id = char2id\n",
    "        self.id2char = id2char\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        sents = [\n",
    "            \"Field &amp; Main Bank purchased a new position in PowerShares Fin . Preferred Port . ( NYSEARCA : PGF )  \"\n",
    "            \"in the fourth quarter , according to its most recent disclosure with the SEC . The institutional investor \"\n",
    "            \"purchased 22,550 shares of the exchange traded fund 's stock , valued at approximately $ 425,000 . \"\n",
    "            \"Other large investors also recently modified their holdings of the company . Cedar Hill Associates LLC \"\n",
    "            \"acquired a new stake in shares of PowerShares Fin . Preferred Port .\",\n",
    "            ]\n",
    "        \n",
    "        for sent in sents:\n",
    "            target = gen_target(sent, self.model, self.char2id, self.id2char, self.maxlen)\n",
    "            print('input:' + sent)\n",
    "            print('output:' + target)\n",
    "        \n",
    "        if logs['val_loss'] <= self.lowest:\n",
    "            self.lowest = logs['val_loss']\n",
    "            self.model.save_weights(self.attn_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsrsTiwqEZeq"
   },
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "vNxh8FEYEZer"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data, path:./data\\train_sample.txt\n",
      "Read data, path:./data\\test_sample.txt\n",
      "Model: \"model_24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_50 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_49 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_24 (Embedding)       (None, None, 128)    1227520     ['input_49[0][0]',               \n",
      "                                                                  'input_50[0][0]']               \n",
      "                                                                                                  \n",
      " gru_98 (GRU)                   (None, None, 128)    99072       ['embedding_24[1][0]']           \n",
      "                                                                                                  \n",
      " bidirectional_48 (Bidirectiona  (None, None, 128)   74496       ['embedding_24[0][0]']           \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " gru_99 (GRU)                   (None, None, 128)    99072       ['gru_98[0][0]']                 \n",
      "                                                                                                  \n",
      " bidirectional_49 (Bidirectiona  (None, None, 128)   74496       ['bidirectional_48[0][0]']       \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " lambda_96 (Lambda)             (None, None, 1)      0           ['input_49[0][0]']               \n",
      "                                                                                                  \n",
      " interact_24 (Interact)         (None, None, 384)    16384       ['gru_99[0][0]',                 \n",
      "                                                                  'bidirectional_49[0][0]',       \n",
      "                                                                  'lambda_96[0][0]']              \n",
      "                                                                                                  \n",
      " dense_48 (Dense)               (None, None, 512)    197120      ['interact_24[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_98 (Lambda)             (None, 1, 9590)      0           ['input_49[0][0]',               \n",
      "                                                                  'lambda_96[0][0]']              \n",
      "                                                                                                  \n",
      " dense_49 (Dense)               (None, None, 9590)   4919670     ['dense_48[0][0]']               \n",
      "                                                                                                  \n",
      " scale_shift_24 (ScaleShift)    (None, 1, 9590)      19180       ['lambda_98[0][0]']              \n",
      "                                                                                                  \n",
      " lambda_99 (Lambda)             (None, None, 9590)   0           ['dense_49[0][0]',               \n",
      "                                                                  'scale_shift_24[0][0]']         \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, None, 9590)   0           ['lambda_99[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_96 (S  (None, None)        0           ['input_50[0][0]']               \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_97 (S  (None, None, 9590)  0           ['activation_24[0][0]']          \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " lambda_97 (Lambda)             (None, None, 1)      0           ['input_50[0][0]']               \n",
      "                                                                                                  \n",
      " tf.keras.backend.sparse_catego  (None, None)        0           ['tf.__operators__.getitem_96[0][\n",
      " rical_crossentropy_24 (TFOpLam                                  0]',                             \n",
      " bda)                                                             'tf.__operators__.getitem_97[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_98 (S  (None, None)        0           ['lambda_97[0][0]']              \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.math.multiply_24 (TFOpLambd  (None, None)        0           ['tf.keras.backend.sparse_categor\n",
      " a)                                                              ical_crossentropy_24[0][0]',     \n",
      "                                                                  'tf.__operators__.getitem_98[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_99 (S  (None, None)        0           ['lambda_97[0][0]']              \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_48 (TFOpLam  ()                  0           ['tf.math.multiply_24[0][0]']    \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_49 (TFOpLam  ()                  0           ['tf.__operators__.getitem_99[0][\n",
      " bda)                                                            0]']                             \n",
      "                                                                                                  \n",
      " tf.math.truediv_24 (TFOpLambda  ()                  0           ['tf.math.reduce_sum_48[0][0]',  \n",
      " )                                                                'tf.math.reduce_sum_49[0][0]']  \n",
      "                                                                                                  \n",
      " add_loss_24 (AddLoss)          ()                   0           ['tf.math.truediv_24[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,727,010\n",
      "Trainable params: 6,727,010\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\sujal\\Desktop\\machine_learning\\project1\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\sujal\\Desktop\\machine_learning\\project1\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\sujal\\Desktop\\machine_learning\\project1\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\sujal\\Desktop\\machine_learning\\project1\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\sujal\\Desktop\\machine_learning\\project1\\env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\sujal\\Desktop\\machine_learning\\project1\\env\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_24\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, None) dtype=int32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [34], line 82\u001b[0m\n\u001b[0;32m     80\u001b[0m sv\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocab.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     81\u001b[0m at_model\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn_model.weight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 82\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtr_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtest_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mts_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m      \u001b[49m\u001b[43msave_vocab_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattn_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mat_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m      \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m      \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [34], line 70\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_path, test_path, save_vocab_path, attn_model_path, batch_size, epochs, maxlen, hidden_dim, min_count, dropout, use_gpu, sep)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msummary())\n\u001b[0;32m     69\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m Evaluate(model, attn_model_path, token_2_id, id_2_token, maxlen)\n\u001b[1;32m---> 70\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_2_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_texts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_validation_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_target_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_2_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\machine_learning\\project1\\env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileft7xkwqt.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\sujal\\Desktop\\machine_learning\\project1\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\sujal\\Desktop\\machine_learning\\project1\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\sujal\\Desktop\\machine_learning\\project1\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\sujal\\Desktop\\machine_learning\\project1\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\sujal\\Desktop\\machine_learning\\project1\\env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\sujal\\Desktop\\machine_learning\\project1\\env\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_24\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, None) dtype=int32>]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from corpus_reader import CorpusReader, str2id, padding, load_word_dict, save_word_dict\n",
    "# from generator.evaluate import Evaluate\n",
    "# from generator import config\n",
    "# from generator.seq2seq_attn_model import Seq2seqAttnModel\n",
    "\n",
    "\n",
    "def data_generator(input_texts, target_texts, char2id, batch_size, maxlen=400):\n",
    "    while True:\n",
    "        X, Y = [], []\n",
    "        for i in range(len(input_texts)):\n",
    "            X.append(str2id(input_texts[i], char2id, maxlen))\n",
    "            Y.append(str2id(target_texts[i], char2id, maxlen))\n",
    "            if len(X) == batch_size:\n",
    "                X = np.array(padding(X, char2id))\n",
    "                Y = np.array(padding(Y, char2id))\n",
    "                yield [X, Y], None\n",
    "                X, Y = [], []\n",
    "\n",
    "\n",
    "def get_validation_data(input_texts, target_texts, char2id, maxlen=400):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(input_texts)):\n",
    "        X.append(str2id(input_texts[i], char2id, maxlen))\n",
    "        Y.append(str2id(target_texts[i], char2id, maxlen))\n",
    "        X = np.array(padding(X, char2id))\n",
    "        Y = np.array(padding(Y, char2id))\n",
    "        return [X, Y], None\n",
    "\n",
    "\n",
    "def train(train_path='',\n",
    "          test_path='',\n",
    "          save_vocab_path='',\n",
    "          attn_model_path='',\n",
    "          batch_size=64,\n",
    "          epochs=100,\n",
    "          maxlen=400,\n",
    "          hidden_dim=128,\n",
    "          min_count=5,\n",
    "          dropout=0.2,\n",
    "          use_gpu=False,\n",
    "          sep='\\t'):\n",
    "    # load or save word dict\n",
    "    if os.path.exists(save_vocab_path):\n",
    "        token_2_id = load_word_dict(save_vocab_path)\n",
    "        data_reader = CorpusReader(train_path=train_path, token_2_id=token_2_id, min_count=min_count, sep=sep)\n",
    "    else:\n",
    "        print('Training data...')\n",
    "        data_reader = CorpusReader(train_path=train_path, min_count=min_count, sep=sep)\n",
    "        token_2_id = data_reader.token_2_id\n",
    "        save_word_dict(token_2_id, save_vocab_path)\n",
    "\n",
    "    id_2_token = data_reader.id_2_token\n",
    "    input_texts, target_texts = data_reader.build_dataset(train_path)\n",
    "    test_input_texts, test_target_texts = data_reader.build_dataset(test_path)\n",
    "\n",
    "    model = Seq2seqAttnModel(token_2_id,\n",
    "                             attn_model_path=attn_model_path,\n",
    "                             hidden_dim=hidden_dim,\n",
    "                             use_gpu=use_gpu,\n",
    "                             dropout=dropout).build_model()\n",
    "    print(model.summary())\n",
    "    evaluator = Evaluate(model, attn_model_path, token_2_id, id_2_token, maxlen)\n",
    "    model.fit(data_generator(input_texts, target_texts, token_2_id, batch_size, maxlen),\n",
    "                        steps_per_epoch=(len(input_texts) + batch_size - 1) // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=get_validation_data(test_input_texts, test_target_texts, token_2_id, maxlen),\n",
    "                        callbacks=[evaluator])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tr_path=os.path.join(data_dir, 'train_sample.txt')\n",
    "    ts_path=os.path.join(data_dir, 'test_sample.txt')\n",
    "    sv=os.path.join(output_dir, 'vocab.txt')\n",
    "    at_model=os.path.join(output_dir, 'attn_model.weight')\n",
    "    train(train_path=tr_path,\n",
    "          test_path=ts_path,\n",
    "          save_vocab_path=sv,\n",
    "          attn_model_path=at_model,\n",
    "          batch_size=32,\n",
    "          epochs=50,\n",
    "          maxlen=400,\n",
    "          hidden_dim=128,\n",
    "          min_count=5,\n",
    "          dropout=0.0,\n",
    "          use_gpu=False,\n",
    "          sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjFQ8bdbEdqt"
   },
   "source": [
    "## Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "_U2gkAwuEk9G"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import necessary dependencies here\n",
    "# from generator import config\n",
    "# from generator.corpus_reader import load_word_dict\n",
    "# from generator.evaluate import gen_target\n",
    "# from generator.seq2seq_attn_model import Seq2seqAttnModel\n",
    "\n",
    "\n",
    "class Inference(object):\n",
    "    def __init__(self, save_vocab_path='', attn_model_path='', maxlen=400):\n",
    "        if os.path.exists(save_vocab_path):\n",
    "            self.char2id = load_word_dict(save_vocab_path)\n",
    "            self.id2char = {int(j): i for i, j in self.char2id.items()}\n",
    "            self.chars = set(self.char2id.keys())\n",
    "        else:\n",
    "            print('Vocabulary path does not exist.')\n",
    "\n",
    "        # Initialize other attributes and load the model\n",
    "        self.maxlen = maxlen\n",
    "        self.model = Seq2seqAttnModel(self.chars, attn_model_path=attn_model_path).build_model()\n",
    "\n",
    "    def infer(self, sentence):\n",
    "        # Perform inference using the loaded model\n",
    "        return gen_target(sentence, self.model, self.char2id, self.id2char, self.maxlen, topk=3)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inputs = [\n",
    "        \"Field &amp; Main Bank purchased a new position in PowerShares Fin . Preferred Port . ( NYSEARCA : PGF )  \"\n",
    "        \"in the fourth quarter , according to its most recent disclosure with the SEC . The institutional investor \"\n",
    "        \"purchased 22,550 shares of the exchange traded fund 's stock , valued at approximately $ 425,000 . \"\n",
    "        \"Other large investors also recently modified their holdings of the company . Cedar Hill Associates LLC \"\n",
    "        \"acquired a new stake in shares of PowerShares Fin . Preferred Port .\",\n",
    "    ]\n",
    "    \n",
    "    save_vocab_path = 'path_to_vocab_file'\n",
    "    attn_model_path = 'path_to_model_file'\n",
    "    maxlen = 400\n",
    "\n",
    "    inference = Inference(save_vocab_path=save_vocab_path, attn_model_path=attn_model_path, maxlen=maxlen)\n",
    "    for i in inputs:\n",
    "        target = inference.infer(i)\n",
    "        print('input:' + i)\n",
    "        print('output:' + target)\n",
    "\n",
    "    while True:\n",
    "        sent = input('input:')\n",
    "        print(\"output:\" + inference.infer(sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
